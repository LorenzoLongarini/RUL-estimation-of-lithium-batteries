{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "root = '../dataset/hi'\n",
    "root_soh = '../dataset/soh_charge'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          hi_v\n",
      "0    25.486136\n",
      "1    23.861111\n",
      "2    23.865501\n",
      "3    23.032055\n",
      "4    22.778535\n",
      "..         ...\n",
      "792   4.188914\n",
      "793   4.188642\n",
      "794   4.065530\n",
      "795   3.943356\n",
      "796   4.067687\n",
      "\n",
      "[797 rows x 1 columns]\n",
      "          SOH\n",
      "0    1.156806\n",
      "1    1.132868\n",
      "2    1.132423\n",
      "3    1.132210\n",
      "4    1.127613\n",
      "..        ...\n",
      "792  0.311893\n",
      "793  0.312044\n",
      "794  0.308795\n",
      "795  0.303900\n",
      "796  0.305063\n",
      "\n",
      "[797 rows x 1 columns]\n",
      "          hi_v\n",
      "0    25.531173\n",
      "1    23.757588\n",
      "2    23.858216\n",
      "3    23.000478\n",
      "4    22.899201\n",
      "..         ...\n",
      "822   1.336601\n",
      "823   1.242275\n",
      "824   1.413041\n",
      "825   1.454523\n",
      "826   1.436719\n",
      "\n",
      "[827 rows x 1 columns]\n",
      "          SOH\n",
      "0    1.159089\n",
      "1    1.139739\n",
      "2    1.139088\n",
      "3    1.138794\n",
      "4    1.136330\n",
      "..        ...\n",
      "822  0.159022\n",
      "823  0.159606\n",
      "824  0.162734\n",
      "825  0.164879\n",
      "826  0.163638\n",
      "\n",
      "[827 rows x 1 columns]\n",
      "          hi_v\n",
      "0    25.417674\n",
      "1    23.982112\n",
      "2    24.101577\n",
      "3    23.153644\n",
      "4    23.027349\n",
      "..         ...\n",
      "899   3.576621\n",
      "900   3.576648\n",
      "901   3.699357\n",
      "902   3.578159\n",
      "903   3.574943\n",
      "\n",
      "[904 rows x 1 columns]\n",
      "          SOH\n",
      "0    1.160882\n",
      "1    1.134520\n",
      "2    1.133957\n",
      "3    1.133369\n",
      "4    1.132094\n",
      "..        ...\n",
      "899  0.289788\n",
      "900  0.289421\n",
      "901  0.294550\n",
      "902  0.287118\n",
      "903  0.290654\n",
      "\n",
      "[904 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_process(root, root_soh, dataset):\n",
    "    hiv = pd.read_csv(f'{root}/hiv-charge-CS2_{dataset}.csv')\n",
    "    soh = pd.read_csv(f'{root_soh}/soh-CS2_{dataset}.csv')\n",
    "\n",
    "    # Mantieni solo le colonne da hiv e soh\n",
    "    df = pd.merge(hiv, soh, how=\"right\", on=\"cycle\")[[\"cycle\", \"hi_v\", \"SOH\"]]\n",
    "    return df\n",
    "\n",
    "# Carica e processa i dataset\n",
    "df_35 = load_and_process(root, root_soh, 35)\n",
    "df_36 = load_and_process(root, root_soh, 36)\n",
    "df_38 = load_and_process(root, root_soh, 38)\n",
    "\n",
    "# Crea le variabili df serparate per ogni dataset, hiv e soh separate\n",
    "df_35_hiv = df_35[[\"cycle\", \"hi_v\"]]\n",
    "df_35_soh = df_35[[\"cycle\", \"SOH\"]]\n",
    "df_36_hiv = df_36[[\"cycle\", \"hi_v\"]]\n",
    "df_36_soh = df_36[[\"cycle\", \"SOH\"]]\n",
    "df_38_hiv = df_38[[\"cycle\", \"hi_v\"]]\n",
    "df_38_soh = df_38[[\"cycle\", \"SOH\"]]\n",
    "\n",
    "# Elimina la colonna cycle dai df\n",
    "df_35_hiv = df_35_hiv.drop(columns=[\"cycle\"])\n",
    "df_35_soh = df_35_soh.drop(columns=[\"cycle\"])\n",
    "df_36_hiv = df_36_hiv.drop(columns=[\"cycle\"])\n",
    "df_36_soh = df_36_soh.drop(columns=[\"cycle\"])\n",
    "df_38_hiv = df_38_hiv.drop(columns=[\"cycle\"])\n",
    "df_38_soh = df_38_soh.drop(columns=[\"cycle\"])\n",
    "\n",
    "\n",
    "# Salva i dataset\n",
    "# df_35_hiv.to_csv(f'{root}/hiv_35.csv', index=False)\n",
    "# df_35_soh.to_csv(f'{root_soh}/soh_35.csv', index=False)\n",
    "# df_36_hiv.to_csv(f'{root}/hiv_36.csv', index=False)\n",
    "# df_36_soh.to_csv(f'{root_soh}/soh_36.csv', index=False)\n",
    "# df_38_hiv.to_csv(f'{root}/hiv_38.csv', index=False)\n",
    "# df_38_soh.to_csv(f'{root_soh}/soh_38.csv', index=False)\n",
    "\n",
    "# Stampa i dataset\n",
    "print(df_35_hiv)\n",
    "print(df_35_soh)\n",
    "print(df_36_hiv)\n",
    "print(df_36_soh)\n",
    "print(df_38_hiv)\n",
    "print(df_38_soh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(data, seq_length):\n",
    "    X, y = [],[]\n",
    "    \n",
    "    for j in data:\n",
    "        for i in range(len(j) - seq_length - 1):\n",
    "            X.append(j[i:i+seq_length])\n",
    "            y.append(j[i + 1:i+seq_length + 1])\n",
    "        \n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1713, 10, 1)\n",
      "(1713, 10, 1)\n",
      "(1713, 10, 1)\n",
      "(788, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "\n",
    "def split_sequence(dataframes, seq_length):\n",
    "    sequences = []\n",
    "    for df in dataframes:\n",
    "        data = df.values\n",
    "        for i in range(len(data) - seq_length + 1):\n",
    "            seq = data[i:i + seq_length, :]\n",
    "            sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "# Assuming df_36_hiv, df_38_hiv, df_36_soh, df_38_soh, df_35_soh are defined\n",
    "\n",
    "# Create sequences for training set\n",
    "X_train_hiv = split_sequence([df_36_hiv, df_38_hiv], seq_length)\n",
    "y_train_soh = split_sequence([df_36_soh, df_38_soh], seq_length)\n",
    "\n",
    "print(X_train_hiv.shape)\n",
    "\n",
    "\n",
    "# Calculate mean and std for normalization\n",
    "mean = X_train_hiv.mean()\n",
    "std = X_train_hiv.std()\n",
    "mean1 = y_train_soh.mean()\n",
    "std1 = y_train_soh.std()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train_hiv - mean) / std\n",
    "y_train = (y_train_soh - mean1) / std1\n",
    "\n",
    "# Shuffle the sequences\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "# Create sequences for testing set\n",
    "X_test = split_sequence([df_35_soh], seq_length)\n",
    "y_test = split_sequence([df_35_soh], seq_length)\n",
    "\n",
    "# Normalize the test data using the mean and std from training data\n",
    "X_test = (X_test - mean) / std\n",
    "y_test = (y_test - mean1) / std1\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1713, 10, 1) 10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "n_steps = 10\n",
    "n_features = 2\n",
    "\n",
    "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "print(X_train.shape, n_steps)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.00005, beta_1=0.9, beta_2=0.999)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', input_shape=(n_steps, n_features), return_sequences= False))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_33' (type Sequential).\n    \n    Input 0 of layer \"lstm_33\" is incompatible with the layer: expected shape=(None, None, 2), found shape=(None, 10, 1)\n    \n    Call arguments received by layer 'sequential_33' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 10, 1), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/jl/gbnfw2jj64l1rzxkswk6bwgr0000gn/T/__autograph_generated_file6b_dbtgj.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_33' (type Sequential).\n    \n    Input 0 of layer \"lstm_33\" is incompatible with the layer: expected shape=(None, None, 2), found shape=(None, 10, 1)\n    \n    Call arguments received by layer 'sequential_33' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 10, 1), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Funzione per preparare i dati per l'addestramento del modello LSTM\n",
    "def prepare_data(df, look_back=1):\n",
    "    data = df.values\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    x, y = [], []\n",
    "    for i in range(len(data_scaled) - look_back):\n",
    "        x.append(data_scaled[i:(i + look_back), 0])\n",
    "        y.append(data_scaled[i + look_back, 1])\n",
    "\n",
    "    return np.array(x), np.array(y), scaler\n",
    "\n",
    "# Caricamento e pre-processing dei dati di addestramento (df_36_38)\n",
    "look_back = 10  # Imposta la dimensione della finestra temporale per la predizione\n",
    "x_train, y_train, scaler_train = prepare_data(df_36_38, look_back)\n",
    "\n",
    "# Reshape per input LSTM [samples, time steps, features]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# Creazione del modello LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(look_back, 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Addestramento del modello\n",
    "model.fit(x_train, y_train, epochs=1000, batch_size=128, verbose=1)\n",
    "\n",
    "# Preparazione dei dati di test (df_35)\n",
    "x_test, y_test, scaler_test = prepare_data(df_35, look_back)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Predizione utilizzando il modello addestrato\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Rescala i dati predetti utilizzando lo scaler dei dati di test\n",
    "y_pred = scaler_test.inverse_transform(np.concatenate((x_test[:, -1, :], y_pred.reshape(-1, 1)), axis=1))[:, 1]\n",
    "\n",
    "# Plot della predizione\n",
    "plt.plot(df_35[\"SOH\"].values, label='True SOH')\n",
    "plt.plot(y_pred, label='Predicted SOH')\n",
    "plt.legend()\n",
    "plt.title('LSTM Model Prediction on Test Data')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
